{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "from os import listdir\n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "from os.path import isfile, join \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import pickle as pkl\n",
    "from scipy import sparse as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from typing import *\n",
    "import copy\n",
    "import os\n",
    "from sklearn import metrics  as m\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import scipy as sc\n",
    "import scipy as sc\n",
    "import time\n",
    "import pandas as pd\n",
    "from genpr.tensorflow.io_planetoid_type import load_data\n",
    "from genpr.data.io import load_dataset\n",
    "import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse.linalg import gmres\n",
    "import timeit\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing lxml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Available by link \n",
    "-- https://clinicaltrials.gov/ct2/download_studies?term=COVID&down_chunk=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = os.listdir('covid/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracting description of clinical studies:  \n",
    "    1. ''official\\_title''\n",
    "    2. ''brief\\_summary''\n",
    "    3. ''detailed\\_description''\n",
    "    4.  ''eligibility''  \n",
    "\n",
    "- Labels:\n",
    "    1. ''masking'' \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = []\n",
    "class_= []\n",
    "for i in entries:\n",
    "    tt = \"\"\n",
    "    with open('covid/'+i, \"r\") as file: \n",
    "        content = file.readlines() \n",
    "        content = \"\".join(content)\n",
    "        bs_content = BeautifulSoup(content, \"lxml\")\n",
    "        if bs_content.find(\"official_title\" )!= None :\n",
    "            third_child = bs_content.find(\"official_title\" )\n",
    "            tt+= list( list(third_child.children))[0]\n",
    "        if bs_content.find(\"brief_summary\" )!= None : \n",
    "            third_child = bs_content.find(\"brief_summary\" )\n",
    "            tt+= ' '.join( list(third_child.strings) ).replace('\\n', '')\n",
    "        if bs_content.find(\"detailed_description\" )!= None :\n",
    "            third_child = bs_content.find(\"detailed_description\" ) \n",
    "            tt+=' '.join( list(third_child.strings) ).replace('\\n', '')\n",
    "        if bs_content.find(\"eligibility\" )!= None :\n",
    "            third_child = bs_content.find(\"eligibility\" ) \n",
    "            tt+= ' '.join( list(third_child.strings) ).replace('\\n', '')\n",
    "        if bs_content.find(\"masking\" ) != None:\n",
    "            class_.append(bs_content.find(\"masking\" ).text)\n",
    "        else:\n",
    "            class_.append(0)\n",
    "        text_.append(tt) \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntext = []\n",
    "for t in class_: \n",
    "    if t != 0:\n",
    "        tt = t.split(' ')[0]\n",
    "        if tt == 'None':\n",
    "            tt = t.split(' ')[1].replace(\"(\",\"\")\n",
    "        ntext.append(tt)\n",
    "    else:\n",
    "        ntext.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_[1].split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter NaN masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': text_, 'class':ntext}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_free = df[df['class'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace Not Open mask by Blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_free[\"class\"].replace({\"Double\": \"Blind\", \n",
    "                          \"Quadruple\": \"Blind\",\n",
    "                          \"Single\":\"Blind\", \n",
    "                          \"Triple\":\"Blind\", }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer( stop_words='english', analyzer='word', min_df=5,)\n",
    "X = vectorizer.fit_transform(df_free['text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "Y = le.fit_transform(df_free['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2039/2039 [00:00<00:00, 177814.91it/s]\n"
     ]
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=7, metric='dice', )\n",
    "neigh.fit(X.toarray())\n",
    "distances, indices = neigh.kneighbors(X.toarray())\n",
    "Af = np.zeros((X.shape[0], X.shape[0]))\n",
    "for i in tqdm.tqdm(range(X.shape[0])):\n",
    "    Af[i, indices[i]] = 1\n",
    "    Af[ indices[i], i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data as in \n",
    "## Revisiting Semi-Supervised Learning with Graph Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_idx(idx: np.ndarray, idx_exclude_list: List[np.ndarray]) -> np.ndarray:\n",
    "    idx_exclude = np.concatenate(idx_exclude_list)\n",
    "    return np.array([i for i in idx if i not in idx_exclude])\n",
    "  \n",
    "def known_unknown_split(idx: np.ndarray, nknown, seed) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    known_idx = rnd_state.choice(idx, nknown, replace=False)\n",
    "    unknown_idx = exclude_idx(idx, [known_idx])\n",
    "    return known_idx, unknown_idx\n",
    "  \n",
    "def train_stopping_split( idx: np.ndarray, labels: np.ndarray, ntrain_per_class, nstopping, seed, ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    train_idx_split = []\n",
    "    for i in np.unique(labels):\n",
    "        train_idx_split.append(\n",
    "            rnd_state.choice(idx[labels == i], ntrain_per_class, replace=False)\n",
    "        )\n",
    "    train_idx = np.concatenate(train_idx_split)\n",
    "    stopping_idx = rnd_state.choice(\n",
    "        exclude_idx(idx, [train_idx]), nstopping, replace=False\n",
    "    )\n",
    "    return train_idx, stopping_idx\n",
    "  \n",
    "def gen_splits(labels: np.ndarray, idx_split_args: Dict[str, int], test ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    all_idx = np.arange(len(labels))\n",
    "    known_idx, unknown_idx = known_unknown_split(\n",
    "        all_idx, idx_split_args[\"nknown\"], idx_split_args[\"seed\"]\n",
    "    )\n",
    "    _, cnts = np.unique(labels[known_idx], return_counts=True)\n",
    "    stopping_split_args = copy.copy(idx_split_args)\n",
    "    del stopping_split_args[\"nknown\"]\n",
    "    train_idx, stopping_idx = train_stopping_split(\n",
    "        known_idx, labels[known_idx], **stopping_split_args\n",
    "    )\n",
    "    if test:\n",
    "        val_idx = unknown_idx\n",
    "    else:\n",
    "        val_idx = exclude_idx(known_idx, [train_idx, stopping_idx])\n",
    "    return train_idx, stopping_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split_args = {\n",
    "    \"ntrain_per_class\": 20,\n",
    "    \"nstopping\": 500,\n",
    "    \"nknown\": 1362,\n",
    "    \"seed\": 0,\n",
    "}\n",
    " \n",
    "train_idx, _, test_idx = gen_splits(\n",
    "    labels=Y, idx_split_args=idx_split_args, test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm = X.toarray().T\n",
    "Xm = Xm - np.median(Xm, axis=0)\n",
    "MMx = np.dot(Xm.T,Xm ) /(Xm.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = train_idx \n",
    "y_all = np.zeros((X.shape[0], 2))\n",
    "for i in new_labels: \n",
    "    c_ = int(Y[i] )  \n",
    "    y_all[i, c_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_A_hat(adj_matrix: sp.spmatrix, delta, sigma, MMx) -> sp.spmatrix:\n",
    "    nnodes = adj_matrix.shape[0]\n",
    "    A = adj_matrix + sp.eye(nnodes)#Ω#@ D_invsqrt_corr\n",
    "    D_vec = np.sum(A, axis=1).A1 \n",
    "    lsigma = sigma -1\n",
    "    rsigma = - sigma\n",
    "    wsigma = -2*sigma + 1\n",
    "    \n",
    "    D_l = sp.diags(np.power(D_vec, lsigma)) \n",
    "    D_r= sp.diags(np.power(D_vec, rsigma ) )\n",
    "    Dw = sp.diags(np.power(D_vec, wsigma ) )\n",
    "    \n",
    "    \n",
    "    return  D_l@A@D_r  +   delta*  MMx@ Dw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosinuse and RBF distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMCos = cosine_similarity(X) /(X.shape[1]-1) \n",
    "MMrbf = rbf_kernel(X) /(X.shape[1] -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmres\n",
      "0.12949191700317897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6115214180206795"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_A_hat(adj_matrix: sp.spmatrix, delta, sigma, MMx) -> sp.spmatrix:\n",
    "    nnodes = adj_matrix.shape[0]\n",
    "    A = adj_matrix + sp.eye(nnodes) \n",
    "    D_vec = np.sum(A, axis=1).A1 \n",
    "    lsigma = sigma - 1\n",
    "    rsigma = - sigma\n",
    "    wsigma = -2*sigma + 1\n",
    "    \n",
    "    D_l = sp.diags(np.power(D_vec, lsigma)) \n",
    "    D_r= sp.diags(np.power(D_vec, rsigma ) )\n",
    "    Dw = sp.diags(np.power(D_vec, wsigma ) )\n",
    "    S_ = MMx@ Dw \n",
    "    \n",
    "    return S_ , D_l@A@D_r  + delta* S_\n",
    "\n",
    "svd_error = 0.1 \n",
    "delta = 1\n",
    "sigma = 1\n",
    "alpha= 0.9\n",
    "svd = TruncatedSVD(n_components=1, algorithm='randomized')\n",
    "mmc, AHAT = calc_A_hat(Af, delta, sigma, MMx ) \n",
    "rex = (np.identity(nnodes)  - alpha * AHAT )\n",
    "svd.fit(mmc)\n",
    "gamma = svd.singular_values_[0]\n",
    "\n",
    "def JOR(A, Z, Y, iter_, beta, alpha):\n",
    "    A = np.copy(A)\n",
    "    Z = np.copy(Z)\n",
    "    Y = np.copy(Y) \n",
    "    for _ in range(iter_):\n",
    "        Z = Z + beta*(alpha * AHAT@Z - Z + (1-alpha) * Y) \n",
    "    return Z\n",
    "\n",
    "def GMRES(A, Y, alpha, k, tol):\n",
    "    A = np.copy(A)\n",
    "    Y = np.copy(Y) \n",
    "    predicts = []\n",
    "    for j in range(k): \n",
    "        temp_ = sparse.linalg.gmres(A, (1-alpha)*Y[:,j], tol=tol)[0] \n",
    "        predicts.append([temp_])\n",
    "    return np.concatenate(predicts).T \n",
    "\n",
    "if (1 + delta* (gamma - svd_error)) <= 1/alpha: \n",
    "    print('jor')\n",
    "    start = timeit.default_timer()\n",
    "    Z = JOR(A=AHAT, Z=y_all, Y=y_all, iter_=15, beta=0.9, alpha=alpha)\n",
    "    print(timeit.default_timer() - start)\n",
    "else:\n",
    "    print('gmres')\n",
    "    start = timeit.default_timer()\n",
    "    Z = GMRES(A=rex, Y=y_all, alpha=alpha, k=y_all.shape[1], tol=1e-03)\n",
    "    print(timeit.default_timer() - start)\n",
    "    \n",
    "m.f1_score( Y[test_idx] ,\n",
    "                  np.argmax(np.array(Z  )[test_idx], axis=-1), average='micro') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhail/opt/anaconda3/lib/python3.7/site-packages/sklearn/semi_supervised/_label_propagation.py:277: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/Users/mikhail/opt/anaconda3/lib/python3.7/site-packages/sklearn/semi_supervised/_label_propagation.py:288: ConvergenceWarning:\n",
      "\n",
      "max_iter=1000 was reached without convergence.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.465288035450517"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "Ytr = np.zeros(Y.shape) -1\n",
    "Ytr[train_idx] = Y[train_idx]\n",
    "label_prop_model = LabelPropagation(kernel='rbf')\n",
    "label_prop_model.fit(X.toarray(), Ytr )\n",
    "Z = label_prop_model.predict(X.toarray())\n",
    "accuracy_score( Y[test_idx], Z[test_idx] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
